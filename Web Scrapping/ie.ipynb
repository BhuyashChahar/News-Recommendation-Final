{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "import html5lib\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"UTA_Enrollment\"]\n",
    "articles = db[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(s):                                             \n",
    "    return re.sub(r'(\\d)(st|nd|rd|th)', r'\\1', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redun_rows(df, default_cols, cont_col_subset = [\"Category\", \"Headline\", \"Summary\", \"Entire_News\", \"News_Link\"], meta_col_subset = [\"Datetime\"]):\n",
    "    \"\"\"It removes faulty or duplicate rows\n",
    "    If there are more columns in the given dataframe than default, this removes those rows that have such more columns. \n",
    "    If there are less columns in the given dataframe than default, it returns \"None\", thereby marking them unusable. \n",
    "    If there are duplicated content in any \"cont_col_subset\", it drops the extra rows except the latest entry.\n",
    "    If there are rows with missing values for important columns, it removes such rows. \n",
    "    If there are rows with missing values for non-important columns, it ignores them, but informs there existence. \n",
    "    It returns the trimmed dataframe as the output in all cases except when the number of columns are less than default,\n",
    "    and prints any missing values in non-important columns\"\"\"\n",
    "    skip = False\n",
    "    if list(df.columns) != list(default_cols):\n",
    "        if len(df.columns) == len(default_cols):\n",
    "            print(\"There seems to be some error in columns names\")\n",
    "        elif len(df.columns) < len(default_cols):\n",
    "            print(\"The given DataFrame seems to have some missing columns\")\n",
    "            df = None # Marking the df useless\n",
    "            skip = True # Skipping the DataFrame operations\n",
    "        elif len(df.columns) > len(default_cols):\n",
    "            print(\"The given DataFrame seems to have more columns than required\")\n",
    "            df_xtra_col_idx = df.loc[:, df.columns[len(default_cols):]].isnull().any(axis = 1)\n",
    "            df = df.loc[df.index[df_xtra_col_idx], default_cols]\n",
    "    if not skip:\n",
    "        df = df.drop_duplicates(subset = cont_col_subset)\n",
    "        df = df.dropna(subset = cont_col_subset+meta_col_subset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1172823309.py, line 198)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 198\u001b[0;36m\u001b[0m\n\u001b[0;31m    # edf1.to_csv('Error_Data_RW.csv', index = False, encoding='utf')\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#looking at the website of indian express,they have a task bar on the first page for broad categories of news\n",
    "#following those broad categories there were some obvious sub-categories where news were covered, these are\n",
    "#mentioned here\n",
    "\n",
    "categories = {\n",
    "    'india':[''],\n",
    "    'cities':[''],\n",
    "    'sports':[\n",
    "         'badminton', 'cricket', 'football', \n",
    "        'fifa', 'hockey', 'motor-sport', 'tennis', \n",
    "        'wwe-wrestling'\n",
    "        ], \n",
    "    'business':[\n",
    "        'aviation', 'banking-and-finance',\n",
    "        'companies', 'economy', 'market'\n",
    "        ],\n",
    "    'technology':[\n",
    "        'gadgets','laptops',\n",
    "        'tech-reviews','science','techook'\n",
    "        ],\n",
    "    'entertainment':['bollywood','hollywood',\n",
    "        'television','tamil','telugu','malayalam',\n",
    "        'web-series','movie-review','box-office-collection'\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "csv_file = 'IndianExpress.csv'\n",
    "\n",
    "ndf1 = pd.DataFrame(columns = [\n",
    "    'Datetime',\n",
    "    'Category',\n",
    "    'Subcategory',\n",
    "    'Headline',\n",
    "    'Summary',\n",
    "    'Entire_News',\n",
    "    'Author',\n",
    "    'News_Link'\n",
    "])\n",
    "\n",
    "\n",
    "ndf1.index.name = \"Index\"\n",
    "\n",
    "\n",
    "#suppose a webpage doesn't exisit for a particular category->sub-category then this data frame will be used to \n",
    "#append that error! page not found type of error\n",
    "\n",
    "edf1 = pd.DataFrame(columns = [\n",
    "    'Website Link',\n",
    "    'Error'\n",
    "])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    news_website_link = \"https://indianexpress.com\"\n",
    "    \n",
    "    for category in categories:\n",
    "        for sub_category in categories[category]:\n",
    "                \n",
    "            for page in range(0,1):  #number of pages to search for\n",
    "                #link for a particular news webpage\n",
    "                news_url = news_website_link + \"/section/\" + category + \"/\" + sub_category + \"/\" + \"page/\" + str(page) + \"/\"\n",
    "                \n",
    "                #scrap the html version of the webpage\n",
    "                sleep(0.05)\n",
    "\n",
    "                try:\n",
    "                    news_html_page = requests.get(news_url)\n",
    "\n",
    "                except Exception as e:\n",
    "                        \n",
    "                        # Pushing the error website link data to the DataFrame\n",
    "                        edf1 = edf1.append({\n",
    "                            'Website Link' : news_urll, \n",
    "                            'Error' : e\n",
    "                        },\n",
    "                        ignore_index = True)\n",
    "                        continue\n",
    "                \n",
    "                #interpret the html file to actual sequence of words\n",
    "                news_html_interpreted = BeautifulSoup(news_html_page.content,'html.parser')\n",
    "                \n",
    "                # Fetching news url for every news on a page from headings and titles\n",
    "                news_head_list = news_html_interpreted.findAll('h2')\n",
    "\n",
    "                print(category)\n",
    "                print(sub_category)\n",
    "                print(page) \n",
    "                \n",
    "                i = 0\n",
    "                for news_head in news_head_list:\n",
    "\n",
    "                    try:\n",
    "                        news_urll = news_head.find('a')['href']\n",
    "\n",
    "                        if articles.find_one({\"News_Link\": news_urll}):\n",
    "                            break    \n",
    "\n",
    "                        news_html_data = requests.get(news_urll)\n",
    "\n",
    "                        news_html_interpretation = BeautifulSoup(news_html_data.content, 'html.parser')\n",
    "\n",
    "                         # Fetching the headline and brief description of the news\n",
    "                        title = news_html_interpretation.find('h1',attrs={'class':'native_story_title'}).get_text()\n",
    "                        summary = news_html_interpretation.find('h2', attrs={'class':'synopsis'}).get_text()\n",
    "\n",
    "                        # Fetching the author, date and time of publish\n",
    "                        publishing_details = news_html_interpretation.find('div', attrs={'class':'editor'})\n",
    "                        publisher = publishing_details.find('a').get_text()\n",
    "                        date_time = publishing_details.find('span').get_text()\n",
    "\n",
    "                    #     dt = date_time.split()\n",
    "                    #     if \"Updated:\" in dt:\n",
    "                    #         dt.remove(\"Updated:\")\n",
    "                    #     # dt.remove(\"Updated:\")\n",
    "                    #     date_time = dt[0] + \" \" + dt[1] + \" \" + dt[2] + \" \" + dt[3] + \" \" + dt[4]\n",
    "                    #     datetime_object = datetime.strptime(solve(date_time), '%B %d, %Y %I:%M:%S %p')\n",
    "                        \n",
    "                        \n",
    "                    #     # Fetching the entire content of the news \n",
    "                    #     report = \"\"\n",
    "                    #     report_paras = news_html_interpretation.findAll('p')\n",
    "                    #     for para in range(len(report_paras)):\n",
    "                    #         report += report_paras[para].get_text()\n",
    "\n",
    "                    #     print(i, end = \" \")\n",
    "                    #     i = i + 1\n",
    "                        \n",
    "                    # except Exception as e:\n",
    "                    #     edf1 = edf1.append({\n",
    "                    #         'Website Link' : news_urll, \n",
    "                    #         'Error' : e\n",
    "                    #     },\n",
    "                    #     ignore_index = True)\n",
    "                    #     continue\n",
    "                    \n",
    "                    # # Pushing the scrapped news data to the DataFrame\n",
    "                    # ndf1 = ndf1.append({\n",
    "                    #     'Datetime' : datetime_object,\n",
    "                    #     # 'Time' : time,\n",
    "                    #     'Category' : category.capitalize(),\n",
    "                    #     'Subcategory' : sub_category.capitalize(),\n",
    "                    #     'Headline' : title,\n",
    "                    #     'Summary' : summary, \n",
    "                    #     'Entire_News' : report,\n",
    "                        # 'Author' : publisher.capitalize(), \n",
    "                        # 'News_Link' : news_urll\n",
    "                        # },\n",
    "                        # ignore_index = True)\n",
    "                        # Handle date formatting\n",
    "                        dt = date_time.split()\n",
    "\n",
    "                        # Remove \"Updated:\" only if it exists\n",
    "                        if \"Updated:\" in dt:\n",
    "                            dt.remove(\"Updated:\")\n",
    "\n",
    "                        date_time = \" \".join(dt[:5])  # Join the first 5 elements back into a string\n",
    "                        datetime_object = datetime.strptime(solve(date_time), '%B %d, %Y %I:%M:%S %p')\n",
    "\n",
    "                        # Create a new DataFrame for the scraped data\n",
    "                        new_data = pd.DataFrame([{\n",
    "                            'Datetime': datetime_object,\n",
    "                            'Category': category.capitalize(),\n",
    "                            'Subcategory': sub_category.capitalize(),\n",
    "                            'Headline': title,\n",
    "                            'Summary': summary,\n",
    "                            'Entire_News': report,\n",
    "                            'Author': publisher.capitalize(),\n",
    "                            'News_Link': news_urll\n",
    "                        }])\n",
    "\n",
    "                        # Concatenate the new data to the existing DataFrame\n",
    "                        ndf1 = pd.concat([ndf1, new_data], ignore_index=True)\n",
    "\n",
    "                        # If there are errors, create a DataFrame and concatenate\n",
    "                        error_data = pd.DataFrame([{\n",
    "                            'Website Link': news_urll,\n",
    "                            'Error': e\n",
    "                        }])\n",
    "\n",
    "                        edf1 = pd.concat([edf1, error_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    # if not ndf1.empty:\n",
    "\n",
    "    #     temp = remove_redun_rows(ndf1, default_cols = ndf1.columns)\n",
    "    #     temp = cat_reformat(temp)\n",
    "\n",
    "    #     # Storing the DataFrame to a .csv file\n",
    "    #     for i in articles.find().sort(\"ID\",-1).limit(1):\n",
    "    #         last_id = int(i[\"ID\"]) + 1\n",
    "    #         temp.index = temp.index + last_id\n",
    "    #     temp.insert(loc=0, column='ID', value=temp.index)\n",
    "    #     data_dict = temp.to_dict(\"records\")\n",
    "    #     articles.insert_many(data_dict)\n",
    "\n",
    "    #     # Storing the unscrapped(error) data to csv\n",
    "        # edf1.to_csv('Error_Data_RW.csv', index = False, encoding='utf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='scraper.log'\n",
    ")\n",
    "\n",
    "class CSVHandler:\n",
    "    def __init__(self, filename: str):\n",
    "        self.filename = filename\n",
    "        self.file_exists = Path(filename).exists()\n",
    "        \n",
    "    def save_to_csv(self, data: pd.DataFrame, mode: str = 'a') -> bool:\n",
    "        try:\n",
    "            # If file doesn't exist, write with header\n",
    "            if not self.file_exists or mode == 'w':\n",
    "                data.to_csv(self.filename, mode='w', index=True, encoding='utf-8')\n",
    "                self.file_exists = True\n",
    "            else:\n",
    "                # Append without header\n",
    "                data.to_csv(self.filename, mode='a', header=False, index=True, encoding='utf-8')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving to CSV {self.filename}: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def backup_existing_file(self) -> None:\n",
    "        if self.file_exists:\n",
    "            backup_name = f\"{self.filename[:-4]}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            try:\n",
    "                os.rename(self.filename, backup_name)\n",
    "                logging.info(f\"Created backup: {backup_name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error creating backup: {str(e)}\")\n",
    "\n",
    "class NewsArticle:\n",
    "    def __init__(self, url: str, category: str, subcategory: str):\n",
    "        self.url = url\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self.html = None\n",
    "        self.soup = None\n",
    "        \n",
    "    def fetch_content(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(self.url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            self.html = response.content\n",
    "            self.soup = BeautifulSoup(self.html, 'html.parser')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching {self.url}: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def parse_article(self) -> Optional[Dict]:\n",
    "        if not self.soup:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Extract article components\n",
    "            title = self.soup.find('h1', attrs={'class': 'native_story_title'})\n",
    "            summary = self.soup.find('h2', attrs={'class': 'synopsis'})\n",
    "            publishing_details = self.soup.find('div', attrs={'class': 'editor'})\n",
    "            \n",
    "            if not all([title, summary, publishing_details]):\n",
    "                return None\n",
    "                \n",
    "            publisher = publishing_details.find('a')\n",
    "            date_time = publishing_details.find('span')\n",
    "            \n",
    "            if not all([publisher, date_time]):\n",
    "                return None\n",
    "                \n",
    "            # Parse datetime\n",
    "            dt_text = date_time.get_text().split()\n",
    "            if \"Updated:\" in dt_text:\n",
    "                dt_text.remove(\"Updated:\")\n",
    "            clean_dt = \" \".join(dt_text[:5])\n",
    "            datetime_obj = datetime.strptime(clean_dt, '%B %d, %Y %I:%M:%S %p')\n",
    "            \n",
    "            # Get full article text\n",
    "            report_paras = self.soup.findAll('p')\n",
    "            report = \" \".join(para.get_text() for para in report_paras)\n",
    "            \n",
    "            return {\n",
    "                'Datetime': datetime_obj,\n",
    "                'Category': self.category.capitalize(),\n",
    "                'Subcategory': self.subcategory.capitalize(),\n",
    "                'Headline': title.get_text().strip(),\n",
    "                'Summary': summary.get_text().strip(),\n",
    "                'Entire_News': report.strip(),\n",
    "                'Author': publisher.get_text().strip().capitalize(),\n",
    "                'News_Link': self.url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing {self.url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "class IndianExpressScraper:\n",
    "    def __init__(self, base_url: str, categories: Dict[str, List[str]], batch_size: int = 10):\n",
    "        self.base_url = base_url\n",
    "        self.categories = categories\n",
    "        self.batch_size = batch_size\n",
    "        self.articles_df = pd.DataFrame(columns=[\n",
    "            'Datetime', 'Category', 'Subcategory', 'Headline',\n",
    "            'Summary', 'Entire_News', 'Author', 'News_Link'\n",
    "        ])\n",
    "        self.errors_df = pd.DataFrame(columns=['Website Link', 'Error'])\n",
    "        self.articles_csv = CSVHandler('IndianExpress.csv')\n",
    "        self.errors_csv = CSVHandler('Error_Data_RW.csv')\n",
    "        \n",
    "    def build_url(self, category: str, subcategory: str, page: int) -> str:\n",
    "        return urljoin(\n",
    "            self.base_url,\n",
    "            f\"/section/{category}/{subcategory}/page/{page}/\"\n",
    "        )\n",
    "        \n",
    "    def save_batch(self) -> None:\n",
    "        \"\"\"Save current batch of articles and clear DataFrame\"\"\"\n",
    "        if not self.articles_df.empty:\n",
    "            self.articles_csv.save_to_csv(self.articles_df)\n",
    "            self.articles_df = pd.DataFrame(columns=self.articles_df.columns)\n",
    "            \n",
    "    def scrape_page(self, url: str, category: str, subcategory: str) -> None:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all news headlines\n",
    "            for news_head in soup.findAll('h2'):\n",
    "                link = news_head.find('a')\n",
    "                if not link or not link.get('href'):\n",
    "                    continue\n",
    "                    \n",
    "                article_url = link['href']\n",
    "                article = NewsArticle(article_url, category, subcategory)\n",
    "                \n",
    "                if article.fetch_content():\n",
    "                    article_data = article.parse_article()\n",
    "                    if article_data:\n",
    "                        self.articles_df = pd.concat([\n",
    "                            self.articles_df,\n",
    "                            pd.DataFrame([article_data])\n",
    "                        ], ignore_index=True)\n",
    "                        logging.info(f\"Successfully scraped: {article_url}\")\n",
    "                        \n",
    "                        # Save batch if we've reached batch_size\n",
    "                        if len(self.articles_df) >= self.batch_size:\n",
    "                            self.save_batch()\n",
    "                            \n",
    "                # Respect the website by adding delay\n",
    "                sleep(0.1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_data = pd.DataFrame([{'Website Link': url, 'Error': str(e)}])\n",
    "            self.errors_csv.save_to_csv(error_data)\n",
    "            logging.error(f\"Error scraping page {url}: {str(e)}\")\n",
    "            \n",
    "    def run(self, pages_per_category: int = 1) -> None:\n",
    "        # Backup existing files\n",
    "        self.articles_csv.backup_existing_file()\n",
    "        self.errors_csv.backup_existing_file()\n",
    "        \n",
    "        for category, subcategories in self.categories.items():\n",
    "            for subcategory in subcategories:\n",
    "                if not subcategory:  # Skip empty subcategories\n",
    "                    continue\n",
    "                    \n",
    "                for page in range(pages_per_category):\n",
    "                    url = self.build_url(category, subcategory, page)\n",
    "                    logging.info(f\"Scraping {category}/{subcategory} - page {page}\")\n",
    "                    self.scrape_page(url, category, subcategory)\n",
    "                    \n",
    "        # Save any remaining articles\n",
    "        self.save_batch()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Categories dictionary\n",
    "    categories = {\n",
    "        'india': [''],\n",
    "        'cities': [''],\n",
    "        'sports': [\n",
    "            'badminton', 'cricket', 'football',\n",
    "            'fifa', 'hockey', 'motor-sport', 'tennis',\n",
    "            'wwe-wrestling'\n",
    "        ],\n",
    "        'business': [\n",
    "            'aviation', 'banking-and-finance',\n",
    "            'companies', 'economy', 'market'\n",
    "        ],\n",
    "        'technology': [\n",
    "            'gadgets', 'laptops',\n",
    "            'tech-reviews', 'science', 'techook'\n",
    "        ],\n",
    "        'entertainment': [\n",
    "            'bollywood', 'hollywood',\n",
    "            'television', 'tamil', 'telugu', 'malayalam',\n",
    "            'web-series', 'movie-review', 'box-office-collection'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Initialize and run scraper\n",
    "    scraper = IndianExpressScraper(\n",
    "        base_url=\"https://indianexpress.com\",\n",
    "        categories=categories,\n",
    "        batch_size=10  # Save to CSV after every 10 articles\n",
    "    )\n",
    "    \n",
    "    scraper.run(pages_per_category=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
