{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These categories will be part of the link as well as the data frames\n",
    "\n",
    "link_variables=[\"news/coronavirus\",\"news/royals\",\"news/crime\",\"news/boris_johnson\",\"news/prince_harry\",\n",
    "                \"tvshowbiz/meghan-markle\",\"sport/premierleague\",\"sport/fa_cup\",\"sport/champions_league\",\n",
    "                \"sport/transfernews\",\"sciencetech/nasa\",\"sciencetech/apple\",\"sciencetech/twitter\",\"sport/kobe_bryant\",\n",
    "                \"tvshowbiz/billie-eilish\",\"tvshowbiz/the-masked-singer-uk\",\"news/donald_trump\",\"news/joe-biden\",\n",
    "                \"news/kamala-harris\",\"news/us-economy\",\"news/dr-anthony-fauci\",\"news/world-health-organization\",\n",
    "                \"news/nhs\", \"news/isis\",\"news/russia\",\"news/iran\",\"news/syria\",\"news/turkey\",\"news/israel\",\"news/china\",\n",
    "                \"news/nigeria\",\"tvshowbiz/adele\",\"tvshowbiz/gogglebox-uk\",\"tvshowbiz/dancing-on-ice\",\"tvshowbiz/bridgerton\",\n",
    "                \"tvshowbiz/kim_kardashian\",\"tvshowbiz/taylor_swift\",\"tvshowbiz/golden_globes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=[]\n",
    "for i in range(0,int(len(link_variables)/19)):\n",
    "    url1.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m latest_news\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfootball-team-news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m other_news\u001b[38;5;241m=\u001b[39m\u001b[43mlatest_news\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle article-small articletext-right\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(other_news)):\n\u001b[1;32m      9\u001b[0m     news_title\u001b[38;5;241m=\u001b[39mother_news[i]\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkro-darkred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for link in url1:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1.to_csv('./' + 'MS18047_Anuraag_Mukherjee.csv', index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2=[]\n",
    "for i in range(int(len(link_variables)/19),2*int(len(link_variables)/19)):\n",
    "    url2.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m latest_news\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfootball-team-news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m other_news\u001b[38;5;241m=\u001b[39m\u001b[43mlatest_news\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle article-small articletext-right\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(other_news)):\n\u001b[1;32m      9\u001b[0m     news_title\u001b[38;5;241m=\u001b[39mother_news[i]\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkro-darkred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for link in url2:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3=[]\n",
    "for i in range(2*int(len(link_variables)/19),3*int(len(link_variables)/19)):\n",
    "    url3.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/prince_harry\n",
      "tvshowbiz/meghan-markle\n"
     ]
    }
   ],
   "source": [
    "for link in url3:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "news3=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "news3.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4=[]\n",
    "for i in range(3*int(len(link_variables)/19),4*int(len(link_variables)/19)):\n",
    "    url4.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport/premierleague\n",
      "sport/fa_cup\n"
     ]
    }
   ],
   "source": [
    "for link in url4:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "news4=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news4.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url5=[]\n",
    "for i in range(4*int(len(link_variables)/19),5*int(len(link_variables)/19)):\n",
    "    url5.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport/champions_league\n",
      "sport/transfernews\n"
     ]
    }
   ],
   "source": [
    "for link in url5:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "news5=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "news5.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url6=[]\n",
    "for i in range(5*int(len(link_variables)/19),6*int(len(link_variables)/19)):\n",
    "    url6.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciencetech/nasa\n",
      "sciencetech/apple\n"
     ]
    }
   ],
   "source": [
    "for link in url6:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "news6=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "news6.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url7=[]\n",
    "for i in range(6*int(len(link_variables)/19),7*int(len(link_variables)/19)):\n",
    "    url7.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m latest_news\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfootball-team-news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m other_news\u001b[38;5;241m=\u001b[39m\u001b[43mlatest_news\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle article-small articletext-right\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(other_news)):\n\u001b[1;32m      9\u001b[0m     news_title\u001b[38;5;241m=\u001b[39mother_news[i]\u001b[38;5;241m.\u001b[39mfind(class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinkro-darkred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "for link in url7:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "news7=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "news7.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url8=[]\n",
    "for i in range(7*int(len(link_variables)/19),8*int(len(link_variables)/19)):\n",
    "    url8.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvshowbiz/billie-eilish\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-8acd317c51c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mauthor\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"author-section byline-plain\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "for link in url8:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "news8=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "news8.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "url9=[]\n",
    "for i in range(8*int(len(link_variables)/19),9*int(len(link_variables)/19)):\n",
    "    url9.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/donald_trump\n",
      "news/joe-biden\n"
     ]
    }
   ],
   "source": [
    "for link in url9:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "news9=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "news9.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "url10=[]\n",
    "for i in range(9*int(len(link_variables)/19),10*int(len(link_variables)/19)):\n",
    "    url10.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/kamala-harris\n",
      "news/us-economy\n"
     ]
    }
   ],
   "source": [
    "for link in url10:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "news10=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "news10.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "url11=[]\n",
    "for i in range(10*int(len(link_variables)/19),11*int(len(link_variables)/19)):\n",
    "    url11.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/dr-anthony-fauci\n",
      "news/world-health-organization\n"
     ]
    }
   ],
   "source": [
    "for link in url11:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "news11=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "news11.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "url12=[]\n",
    "for i in range(11*int(len(link_variables)/19),12*int(len(link_variables)/19)):\n",
    "    url12.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/nhs\n",
      "news/isis\n"
     ]
    }
   ],
   "source": [
    "for link in url12:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "news12=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "news12.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "url13=[]\n",
    "for i in range(12*int(len(link_variables)/19),13*int(len(link_variables)/19)):\n",
    "    url13.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/russia\n",
      "news/iran\n"
     ]
    }
   ],
   "source": [
    "for link in url13:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "news13=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "news13.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "url14=[]\n",
    "for i in range(13*int(len(link_variables)/19),14*int(len(link_variables)/19)):\n",
    "    url14.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/syria\n",
      "news/turkey\n"
     ]
    }
   ],
   "source": [
    "for link in url14:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "news14=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "news14.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "url15=[]\n",
    "for i in range(14*int(len(link_variables)/19),15*int(len(link_variables)/19)):\n",
    "    url15.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/israel\n",
      "news/china\n"
     ]
    }
   ],
   "source": [
    "for link in url15:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "news15=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "news15.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "url16=[]\n",
    "for i in range(15*int(len(link_variables)/19),16*int(len(link_variables)/19)):\n",
    "    url16.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news/nigeria\n",
      "tvshowbiz/adele\n"
     ]
    }
   ],
   "source": [
    "for link in url16:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "news16=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "news16.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "url17=[]\n",
    "for i in range(16*int(len(link_variables)/19),17*int(len(link_variables)/19)):\n",
    "    url17.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvshowbiz/gogglebox-uk\n",
      "tvshowbiz/dancing-on-ice\n"
     ]
    }
   ],
   "source": [
    "for link in url17:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "news17=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "news17.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "url18=[]\n",
    "for i in range(17*int(len(link_variables)/19),18*int(len(link_variables)/19)):\n",
    "    url18.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvshowbiz/bridgerton\n",
      "tvshowbiz/kim_kardashian\n"
     ]
    }
   ],
   "source": [
    "for link in url18:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "news18=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "news18.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these lists will be used to form a DataFrame\n",
    "\n",
    "title=[] # Contains the titles of Articles\n",
    "desc=[]  # Contains a short description of Articles\n",
    "date=[]  # Contains date of article\n",
    "time=[]  # Contains time of article\n",
    "category=[] # Contains category of news\n",
    "subcategory=[] # Contains subcategory of news\n",
    "links=[] #Contains links to article\n",
    "writer=[] #Contains author names\n",
    "full_article=[] #Contains the Full Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "url19=[]\n",
    "for i in range(18*int(len(link_variables)/19),int(len(link_variables))):\n",
    "    url19.append(\"https://www.dailymail.co.uk/\"+link_variables[i]+\"/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tvshowbiz/taylor_swift\n",
      "tvshowbiz/golden_globes\n"
     ]
    }
   ],
   "source": [
    "for link in url19:\n",
    "    page=requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    latest_news=soup.find(class_=\"football-team-news\")\n",
    "    other_news=latest_news.find_all(class_=\"article article-small articletext-right\")\n",
    "    \n",
    "    for i in range(len(other_news)):\n",
    "        news_title=other_news[i].find(class_=\"linkro-darkred\").find(\"a\").get_text()\n",
    "        \n",
    "        news_desc=other_news[i].find_all(\"p\")[2].get_text()\n",
    "        \n",
    "        \n",
    "        date_time= other_news[i].find(class_=\"channel-date-container sport\").get_text()\n",
    "        list1=date_time.split(\" \")\n",
    "        \n",
    "        \n",
    "        cat=link.split(\"/\")\n",
    "        \n",
    "        \n",
    "        link_scrap=other_news[i].find(class_=\"linkro-darkred\").find(\"a\")\n",
    "        l=str(link_scrap)\n",
    "        k=l.split('\"')\n",
    "        \n",
    "        \n",
    "        new_page=requests.get(k[1])\n",
    "        new_soup = BeautifulSoup(new_page.content, 'html.parser')\n",
    "        \n",
    "        article=new_soup.find(id=\"js-article-text\")\n",
    "        \n",
    "        if article==None:\n",
    "            continue\n",
    "    \n",
    "        else:\n",
    "            author= article.find(class_=\"author-section byline-plain\").get_text()\n",
    "            writer.append(author)\n",
    "        \n",
    "            body=article.find(itemprop=\"articleBody\").find_all(\"p\")\n",
    "            art=\"\"\n",
    "            for i in range(len(body)):\n",
    "                full_text=body[i].get_text()\n",
    "                art+=full_text\n",
    "            full_article.append(art)\n",
    "            title.append(news_title)\n",
    "            desc.append(news_desc)\n",
    "            date.append(list1[0])\n",
    "            time.append(list1[1])\n",
    "            category.append(cat[3])\n",
    "            subcategory.append(cat[4])\n",
    "            links.append(k[1])\n",
    "            \n",
    "    print(cat[3]+\"/\"+cat[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "news19=pd.DataFrame({ \"Date\":date, \"Time\":time, \"Category\":category, \"Subcategory\":subcategory, \"Headline\":title, \n",
    "                   \"Summary\":desc, \"Entire_News\": full_article, \"Author\":writer, \"News_Link\":links,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "news19.to_csv('./MS18047_Anuraag_Mukherjee.csv', mode='a', header=False, index = False, encoding='utf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scraped_News = pd.read_csv('./MS18047_Anuraag_Mukherjee.csv')\n",
    "Scraped_News"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
