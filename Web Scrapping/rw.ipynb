{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhuyash./Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "import requests\n",
    "import html5lib\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"UTA_Enrollment\"]\n",
    "articles = db[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(s):                                             \n",
    "    return re.sub(r'(\\d)(st|nd|rd|th)', r'\\1', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redun_rows(df, default_cols, cont_col_subset = [\"Category\", \"Headline\", \"Summary\", \"Entire_News\", \"News_Link\"], meta_col_subset = [\"Datetime\"]):\n",
    "    \"\"\"It removes faulty or duplicate rows\n",
    "    If there are more columns in the given dataframe than default, this removes those rows that have such more columns. \n",
    "    If there are less columns in the given dataframe than default, it returns \"None\", thereby marking them unusable. \n",
    "    If there are duplicated content in any \"cont_col_subset\", it drops the extra rows except the latest entry.\n",
    "    If there are rows with missing values for important columns, it removes such rows. \n",
    "    If there are rows with missing values for non-important columns, it ignores them, but informs there existence. \n",
    "    It returns the trimmed dataframe as the output in all cases except when the number of columns are less than default,\n",
    "    and prints any missing values in non-important columns\"\"\"\n",
    "    skip = False\n",
    "    if list(df.columns) != list(default_cols):\n",
    "        if len(df.columns) == len(default_cols):\n",
    "            print(\"There seems to be some error in columns names\")\n",
    "        elif len(df.columns) < len(default_cols):\n",
    "            print(\"The given DataFrame seems to have some missing columns\")\n",
    "            df = None # Marking the df useless\n",
    "            skip = True # Skipping the DataFrame operations\n",
    "        elif len(df.columns) > len(default_cols):\n",
    "            print(\"The given DataFrame seems to have more columns than required\")\n",
    "            df_xtra_col_idx = df.loc[:, df.columns[len(default_cols):]].isnull().any(axis = 1)\n",
    "            df = df.loc[df.index[df_xtra_col_idx], default_cols]\n",
    "    if not skip:\n",
    "        df = df.drop_duplicates(subset = cont_col_subset)\n",
    "        df = df.dropna(subset = cont_col_subset+meta_col_subset)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_reformat(df):\n",
    "    df['Category'] = df['Category'].replace(['business'],'Business')\n",
    "    df['Category'] = df['Category'].replace(['Business-news'],'Business')\n",
    "    df['Category'] = df['Category'].replace(['India-news'],'India')\n",
    "    df['Category'] = df['Category'].replace(['Sports-news'],'Sports')\n",
    "    df['Category'] = df['Category'].replace(['sports'],'Sports')\n",
    "    df['Category'] = df['Category'].replace(['tvshowbiz'],'Entertainment')\n",
    "    df['Category'] = df['Category'].replace(['entertainment'],'Entertainment')\n",
    "    df['Category'] = df['Category'].replace(['Television'],'Entertainment')\n",
    "    df['Category'] = df['Category'].replace(['Entertainment-news'],'Entertainment')\n",
    "    df['Category'] = df['Category'].replace(['Technology-news'],'Technology')\n",
    "    df['Category'] = df['Category'].replace(['technology'],'Technology')\n",
    "    df['Category'] = df['Category'].replace(['World-news'],'World')\n",
    "    df['Category'] = df['Category'].replace(['news'],'News')\n",
    "    df['Category'] = df['Category'].replace(['society'],'Society')\n",
    "    df['Category'] = df['Category'].replace(['Data-intelligence-unit'],'News')\n",
    "    df['Category'] = df['Category'].replace(['Movies'],'Entertainment')\n",
    "    df['Category'] = df['Category'].replace(['Education-today'],'Education')\n",
    "    df['Category'] = df['Category'].replace(['Cities'],'India')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'india-news':[\n",
    "        'politics', 'general-news', 'elections', \n",
    "        'education', 'economy', 'city-news', 'accidents-and-disasters',\n",
    "         'law-and-order', 'lottery', 'irctc'\n",
    "         ], \n",
    "      'world-news':[\n",
    "          'us-news', 'uk-news',\n",
    "         'pakistan-news', 'europe', 'global-event-news', 'rest-of-the-world-news',\n",
    "         'china', 'australia', 'africa', 'middle-east', 'south-america'\n",
    "         ],\n",
    "     'entertainment-news':[\n",
    "         'bollywood-news','hollywood-news', 'Movie Reviews',\n",
    "         'television-news','regional-indian-cinema','web-series', 'music', 'others', 'rest-of-the-world'\n",
    "         ],\n",
    "     'sports-news':['cricket-news','football-news',\n",
    "         'tennis-news','badminton-news','basketball-news','other-sports',\n",
    "         'kabaddi-news','wwe-news','hockey-news', 'esports'\n",
    "         ],\n",
    "     'technology-news':['gadgets', 'mobile', 'gaming',\n",
    "         'apps', 'e-commerce', 'how-to', 'social-media-news',\n",
    "         'science', 'other-tech-news'\n",
    "         ],\n",
    "     'business-news':['india-business', 'international-business'\n",
    "         ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'RepublicWorld.csv'\n",
    "\n",
    "ndf1 = pd.DataFrame(columns = [\n",
    "    'Datetime',\n",
    "    'Category',\n",
    "    'Subcategory',\n",
    "    'Headline',\n",
    "    'Summary',\n",
    "    'Entire_News',\n",
    "    'Author',\n",
    "    'News_Link'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india-news\n",
      "politics\n",
      "0\n",
      "india-news\n",
      "politics\n",
      "1\n",
      "india-news\n",
      "politics\n",
      "2\n",
      "india-news\n",
      "general-news\n",
      "0\n",
      "india-news\n",
      "general-news\n",
      "1\n",
      "india-news\n",
      "general-news\n",
      "2\n",
      "india-news\n",
      "elections\n",
      "0\n",
      "india-news\n",
      "elections\n",
      "1\n",
      "india-news\n",
      "elections\n",
      "2\n",
      "india-news\n",
      "education\n",
      "0\n",
      "india-news\n",
      "education\n",
      "1\n",
      "india-news\n",
      "education\n",
      "2\n",
      "india-news\n",
      "economy\n",
      "0\n",
      "india-news\n",
      "economy\n",
      "1\n",
      "india-news\n",
      "economy\n",
      "2\n",
      "india-news\n",
      "city-news\n",
      "0\n",
      "india-news\n",
      "city-news\n",
      "1\n",
      "india-news\n",
      "city-news\n",
      "2\n",
      "india-news\n",
      "accidents-and-disasters\n",
      "0\n",
      "india-news\n",
      "accidents-and-disasters\n",
      "1\n",
      "india-news\n",
      "accidents-and-disasters\n",
      "2\n",
      "india-news\n",
      "law-and-order\n",
      "0\n",
      "india-news\n",
      "law-and-order\n",
      "1\n",
      "india-news\n",
      "law-and-order\n",
      "2\n",
      "india-news\n",
      "lottery\n",
      "0\n",
      "india-news\n",
      "lottery\n",
      "1\n",
      "india-news\n",
      "lottery\n",
      "2\n",
      "india-news\n",
      "irctc\n",
      "0\n",
      "india-news\n",
      "irctc\n",
      "1\n",
      "india-news\n",
      "irctc\n",
      "2\n",
      "world-news\n",
      "us-news\n",
      "0\n",
      "world-news\n",
      "us-news\n",
      "1\n",
      "world-news\n",
      "us-news\n",
      "2\n",
      "world-news\n",
      "uk-news\n",
      "0\n",
      "world-news\n",
      "uk-news\n",
      "1\n",
      "world-news\n",
      "uk-news\n",
      "2\n",
      "world-news\n",
      "pakistan-news\n",
      "0\n",
      "world-news\n",
      "pakistan-news\n",
      "1\n",
      "world-news\n",
      "pakistan-news\n",
      "2\n",
      "world-news\n",
      "europe\n",
      "0\n",
      "world-news\n",
      "europe\n",
      "1\n",
      "world-news\n",
      "europe\n",
      "2\n",
      "world-news\n",
      "global-event-news\n",
      "0\n",
      "world-news\n",
      "global-event-news\n",
      "1\n",
      "world-news\n",
      "global-event-news\n",
      "2\n",
      "world-news\n",
      "rest-of-the-world-news\n",
      "0\n",
      "world-news\n",
      "rest-of-the-world-news\n",
      "1\n",
      "world-news\n",
      "rest-of-the-world-news\n",
      "2\n",
      "world-news\n",
      "china\n",
      "0\n",
      "world-news\n",
      "china\n",
      "1\n",
      "world-news\n",
      "china\n",
      "2\n",
      "world-news\n",
      "australia\n",
      "0\n",
      "world-news\n",
      "australia\n",
      "1\n",
      "world-news\n",
      "australia\n",
      "2\n",
      "world-news\n",
      "africa\n",
      "0\n",
      "world-news\n",
      "africa\n",
      "1\n",
      "world-news\n",
      "africa\n",
      "2\n",
      "world-news\n",
      "middle-east\n",
      "0\n",
      "world-news\n",
      "middle-east\n",
      "1\n",
      "world-news\n",
      "middle-east\n",
      "2\n",
      "world-news\n",
      "south-america\n",
      "0\n",
      "world-news\n",
      "south-america\n",
      "1\n",
      "world-news\n",
      "south-america\n",
      "2\n",
      "entertainment-news\n",
      "bollywood-news\n",
      "0\n",
      "entertainment-news\n",
      "bollywood-news\n",
      "1\n",
      "entertainment-news\n",
      "bollywood-news\n",
      "2\n",
      "entertainment-news\n",
      "hollywood-news\n",
      "0\n",
      "entertainment-news\n",
      "hollywood-news\n",
      "1\n",
      "entertainment-news\n",
      "hollywood-news\n",
      "2\n",
      "entertainment-news\n",
      "Movie Reviews\n",
      "0\n",
      "entertainment-news\n",
      "Movie Reviews\n",
      "1\n",
      "entertainment-news\n",
      "Movie Reviews\n",
      "2\n",
      "entertainment-news\n",
      "television-news\n",
      "0\n",
      "entertainment-news\n",
      "television-news\n",
      "1\n",
      "entertainment-news\n",
      "television-news\n",
      "2\n",
      "entertainment-news\n",
      "regional-indian-cinema\n",
      "0\n",
      "entertainment-news\n",
      "regional-indian-cinema\n",
      "1\n",
      "entertainment-news\n",
      "regional-indian-cinema\n",
      "2\n",
      "entertainment-news\n",
      "web-series\n",
      "0\n",
      "entertainment-news\n",
      "web-series\n",
      "1\n",
      "entertainment-news\n",
      "web-series\n",
      "2\n",
      "entertainment-news\n",
      "music\n",
      "0\n",
      "entertainment-news\n",
      "music\n",
      "1\n",
      "entertainment-news\n",
      "music\n",
      "2\n",
      "entertainment-news\n",
      "others\n",
      "0\n",
      "entertainment-news\n",
      "others\n",
      "1\n",
      "entertainment-news\n",
      "others\n",
      "2\n",
      "entertainment-news\n",
      "rest-of-the-world\n",
      "0\n",
      "entertainment-news\n",
      "rest-of-the-world\n",
      "1\n",
      "entertainment-news\n",
      "rest-of-the-world\n",
      "2\n",
      "sports-news\n",
      "cricket-news\n",
      "0\n",
      "sports-news\n",
      "cricket-news\n",
      "1\n",
      "sports-news\n",
      "cricket-news\n",
      "2\n",
      "sports-news\n",
      "football-news\n",
      "0\n",
      "sports-news\n",
      "football-news\n",
      "1\n",
      "sports-news\n",
      "football-news\n",
      "2\n",
      "sports-news\n",
      "tennis-news\n",
      "0\n",
      "sports-news\n",
      "tennis-news\n",
      "1\n",
      "sports-news\n",
      "tennis-news\n",
      "2\n",
      "sports-news\n",
      "badminton-news\n",
      "0\n",
      "sports-news\n",
      "badminton-news\n",
      "1\n",
      "sports-news\n",
      "badminton-news\n",
      "2\n",
      "sports-news\n",
      "basketball-news\n",
      "0\n",
      "sports-news\n",
      "basketball-news\n",
      "1\n",
      "sports-news\n",
      "basketball-news\n",
      "2\n",
      "sports-news\n",
      "other-sports\n",
      "0\n",
      "sports-news\n",
      "other-sports\n",
      "1\n",
      "sports-news\n",
      "other-sports\n",
      "2\n",
      "sports-news\n",
      "kabaddi-news\n",
      "0\n",
      "sports-news\n",
      "kabaddi-news\n",
      "1\n",
      "sports-news\n",
      "kabaddi-news\n",
      "2\n",
      "sports-news\n",
      "wwe-news\n",
      "0\n",
      "sports-news\n",
      "wwe-news\n",
      "1\n",
      "sports-news\n",
      "wwe-news\n",
      "2\n",
      "sports-news\n",
      "hockey-news\n",
      "0\n",
      "sports-news\n",
      "hockey-news\n",
      "1\n",
      "sports-news\n",
      "hockey-news\n",
      "2\n",
      "sports-news\n",
      "esports\n",
      "0\n",
      "sports-news\n",
      "esports\n",
      "1\n",
      "sports-news\n",
      "esports\n",
      "2\n",
      "technology-news\n",
      "gadgets\n",
      "0\n",
      "technology-news\n",
      "gadgets\n",
      "1\n",
      "technology-news\n",
      "gadgets\n",
      "2\n",
      "technology-news\n",
      "mobile\n",
      "0\n",
      "technology-news\n",
      "mobile\n",
      "1\n",
      "technology-news\n",
      "mobile\n",
      "2\n",
      "technology-news\n",
      "gaming\n",
      "0\n",
      "technology-news\n",
      "gaming\n",
      "1\n",
      "technology-news\n",
      "gaming\n",
      "2\n",
      "technology-news\n",
      "apps\n",
      "0\n",
      "technology-news\n",
      "apps\n",
      "1\n",
      "technology-news\n",
      "apps\n",
      "2\n",
      "technology-news\n",
      "e-commerce\n",
      "0\n",
      "technology-news\n",
      "e-commerce\n",
      "1\n",
      "technology-news\n",
      "e-commerce\n",
      "2\n",
      "technology-news\n",
      "how-to\n",
      "0\n",
      "technology-news\n",
      "how-to\n",
      "1\n",
      "technology-news\n",
      "how-to\n",
      "2\n",
      "technology-news\n",
      "social-media-news\n",
      "0\n",
      "technology-news\n",
      "social-media-news\n",
      "1\n",
      "technology-news\n",
      "social-media-news\n",
      "2\n",
      "technology-news\n",
      "science\n",
      "0\n",
      "technology-news\n",
      "science\n",
      "1\n",
      "technology-news\n",
      "science\n",
      "2\n",
      "technology-news\n",
      "other-tech-news\n",
      "0\n",
      "technology-news\n",
      "other-tech-news\n",
      "1\n",
      "technology-news\n",
      "other-tech-news\n",
      "2\n",
      "business-news\n",
      "india-business\n",
      "0\n",
      "business-news\n",
      "india-business\n",
      "1\n",
      "business-news\n",
      "india-business\n",
      "2\n",
      "business-news\n",
      "international-business\n",
      "0\n",
      "business-news\n",
      "international-business\n",
      "1\n",
      "business-news\n",
      "international-business\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ndf1.index.name = \"Index\"\n",
    "\n",
    "\n",
    "#suppose a webpage doesn't exisit for a particular category->sub-category then this data frame will be used to \n",
    "#append that error! page not found type of error\n",
    "\n",
    "edf1 = pd.DataFrame(columns = [\n",
    "    'Website Link',\n",
    "    'Error'\n",
    "])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    news_website_link = \"https://www.republicworld.com/\"\n",
    "    \n",
    "    for category in categories:\n",
    "        for sub_category in categories[category]:\n",
    "                \n",
    "            for page in range(0,3):  #number of pages to search for\n",
    "                #link for a particular news webpage\n",
    "                if page == 0:\n",
    "                    news_url = news_website_link + category + \"/\" + sub_category\n",
    "                else:\n",
    "                    news_url = news_website_link + category + \"/\" + sub_category + \"/\" + str(page)\n",
    "                \n",
    "                #scrap the html version of the webpage\n",
    "                sleep(0.05)\n",
    "\n",
    "                try:\n",
    "                    news_html_page = requests.get(news_url)\n",
    "\n",
    "                except Exception as e:\n",
    "                        \n",
    "                        # Pushing the error website link data to the DataFrame\n",
    "                        edf1 = edf1.append({\n",
    "                            'Website Link' : news_urll, \n",
    "                            'Error' : e\n",
    "                        },\n",
    "                        ignore_index = True)\n",
    "                        continue\n",
    "                \n",
    "                #interpret the html file to actual sequence of words\n",
    "                news_html_interpreted = BeautifulSoup(news_html_page.content,'html.parser')\n",
    "                \n",
    "                # Fetching news url for every news on a page from headings and titles\n",
    "                news_head_list = news_html_interpreted.findAll('article')\n",
    "\n",
    "                print(category)\n",
    "                print(sub_category)\n",
    "                print(page) \n",
    "                \n",
    "                i = 0\n",
    "                for news_head in news_head_list:\n",
    "                    print(i, end = \" \")\n",
    "                    i = i + 1\n",
    "\n",
    "                    news_urll = news_head.find('a')['href']\n",
    "\n",
    "                    if articles.find_one({\"News_Link\": news_urll}):\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        news_html_data = requests.get(news_urll)\n",
    "                        news_html_interpretation = BeautifulSoup(news_html_data.content, 'html.parser')\n",
    "\n",
    "                        # Fetching the headline and brief description of the news\n",
    "                        title = news_html_interpretation.find('h1',attrs={'class':'story-title'}).get_text()\n",
    "                        summary = news_html_interpretation.find('h2', attrs={'class':'story-description'}).get_text()\n",
    "\n",
    "                        # Fetching the author, date and time of publish\n",
    "                        publishing_details = news_html_interpretation.find('div', attrs={'class':'author'})\n",
    "                        publisher = publishing_details.find('span').get_text()\n",
    "                        publisher = publisher.strip()\n",
    "                        date_time = news_html_interpretation.find('time').get_text()\n",
    "                        dt_obj = datetime.strptime(solve(date_time), '%d %B, %Y %H:%M IST')\n",
    "                        datetime_object = dt_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        \n",
    "                        # # Processing and separating date and time\n",
    "                        # #date_time = date_time.replace('Updated:', '')\n",
    "                        # #date_time = date_time.strip()\n",
    "                        # date_time = date_time.split()\n",
    "                        # date = date_time[0] + \" \" + date_time[1] + \" \" + date_time[2]\n",
    "                        # time = date_time[3] + \" \" + date_time[4]\n",
    "\n",
    "                        # Fetching the entire content of the news \n",
    "                        report = \"\"\n",
    "                        report_section = news_html_interpretation.find('div', attrs={'class':'width100 storytext'})\n",
    "                        report_paras = report_section.findAll('p')\n",
    "                        for para in range(len(report_paras)):\n",
    "                            report += report_paras[para].get_text()\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        edf1 = edf1.append({\n",
    "                            'Website Link' : news_urll, \n",
    "                            'Error' : e\n",
    "                        },\n",
    "                        ignore_index = True)\n",
    "                        continue\n",
    "                    \n",
    "                    # Pushing the scrapped news data to the DataFrame\n",
    "                    ndf1 = ndf1.append({\n",
    "                        'Datetime' : datetime_object,\n",
    "                        # 'Time' : time,\n",
    "                        'Category' : category.capitalize(),\n",
    "                        'Subcategory' : sub_category.capitalize(),\n",
    "                        'Headline' : title,\n",
    "                        'Summary' : summary, \n",
    "                        'Entire_News' : report,\n",
    "                        'Author' : publisher.capitalize(), \n",
    "                        'News_Link' : news_urll\n",
    "                        },\n",
    "                        ignore_index = True)\n",
    "\n",
    "    if not ndf1.empty:\n",
    "\n",
    "        temp = remove_redun_rows(ndf1, default_cols = ndf1.columns)\n",
    "        temp = cat_reformat(temp)\n",
    "\n",
    "        # Storing the DataFrame to a .csv file\n",
    "        data_dict = temp.to_dict(\"records\")\n",
    "        articles.insert_many(data_dict)\n",
    "\n",
    "        # Storing the unscrapped(error) data to csv\n",
    "        edf1.to_csv('Error_Data_RW.csv', index = False, encoding='utf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
