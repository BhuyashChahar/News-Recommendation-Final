{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from datasketch import MinHashLSHForest, MinHash\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOT-2 Function (Content-based filtering and Collabartive filtering function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsRecommendationBot:\n",
    "    def __init__(self, user_data_path: str, process_data_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation bot with data paths and necessary components\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Simple stopwords list\n",
    "        self.stop_words = {\n",
    "            'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', \n",
    "            'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the', \n",
    "            'to', 'was', 'were', 'will', 'with'\n",
    "        }\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        self._load_and_preprocess_data(user_data_path, process_data_path)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.logger.info(\"Initializing components...\")\n",
    "        self.vectorizer = HashingVectorizer(n_features=1000)\n",
    "        self.lsh_forest = MinHashLSHForest(num_perm=128)\n",
    "        \n",
    "        # Prepare data structures\n",
    "        self.logger.info(\"Preparing data structures...\")\n",
    "        self._prepare_data()\n",
    "        self.logger.info(\"Initialization complete!\")\n",
    "        \n",
    "        # Print debug information\n",
    "        self._print_debug_info()\n",
    "    \n",
    "    def _load_and_preprocess_data(self, user_data_path: str, process_data_path: str):\n",
    "        \"\"\"Load and preprocess the data files\"\"\"\n",
    "        self.logger.info(\"Loading data...\")\n",
    "        try:\n",
    "            # Load data\n",
    "            self.user_data = pd.read_csv(user_data_path)\n",
    "            self.process_data = pd.read_csv(process_data_path)\n",
    "            \n",
    "            # Convert UserId to int if it's not already\n",
    "            self.user_data['UserId'] = pd.to_numeric(self.user_data['UserId'], errors='coerce')\n",
    "            \n",
    "            # Remove any rows with NaN UserIds\n",
    "            self.user_data = self.user_data.dropna(subset=['UserId'])\n",
    "            \n",
    "            # Convert UserId to int\n",
    "            self.user_data['UserId'] = self.user_data['UserId'].astype(int)\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(self.user_data)} user interactions and {len(self.process_data)} articles\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _print_debug_info(self):\n",
    "        \"\"\"Print debug information about the loaded data\"\"\"\n",
    "        self.logger.info(\"\\nDebug Information:\")\n",
    "        self.logger.info(f\"Number of unique users: {len(self.user_data['UserId'].unique())}\")\n",
    "        self.logger.info(f\"User ID range: {self.user_data['UserId'].min()} to {self.user_data['UserId'].max()}\")\n",
    "        self.logger.info(f\"User ID data type: {self.user_data['UserId'].dtype}\")\n",
    "        self.logger.info(f\"Number of articles: {len(self.process_data)}\")\n",
    "        self.logger.info(\"\\nFirst few User IDs:\")\n",
    "        self.logger.info(self.user_data['UserId'].head())\n",
    "    \n",
    "    def _simple_tokenize(self, text):\n",
    "        \"\"\"Simple tokenization function\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', str(text).lower())\n",
    "        tokens = [word for word in text.split() if word not in self.stop_words and len(word) > 2]\n",
    "        return tokens\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare data structures and indexes\"\"\"\n",
    "        # Create article content matrix\n",
    "        self.article_content = (\n",
    "            self.process_data['headline'].fillna('') + ' ' +\n",
    "            self.process_data['category'].fillna('') + ' ' +\n",
    "            self.process_data['subcategory'].fillna('') + ' ' +\n",
    "            self.process_data['Entire_News'].fillna('')\n",
    "        )\n",
    "        \n",
    "        # Create MinHash for each article\n",
    "        self.minhashes = {}\n",
    "        total_articles = len(self.article_content)\n",
    "        \n",
    "        self.logger.info(f\"Processing {total_articles} articles...\")\n",
    "        for idx, content in enumerate(self.article_content):\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                self.logger.info(f\"Processed {idx + 1}/{total_articles} articles\")\n",
    "            \n",
    "            tokens = self._simple_tokenize(content)\n",
    "            minhash = MinHash(num_perm=128)\n",
    "            for token in tokens:\n",
    "                minhash.update(token.encode('utf-8'))\n",
    "            self.minhashes[idx] = minhash\n",
    "            self.lsh_forest.add(str(idx), minhash)\n",
    "        \n",
    "        self.lsh_forest.index()\n",
    "        \n",
    "        # Create user-article interaction matrix\n",
    "        self.logger.info(\"Creating user-article interaction matrix...\")\n",
    "        self.user_article_matrix = pd.pivot_table(\n",
    "            self.user_data,\n",
    "            values='TimeSpent',\n",
    "            index='UserId',\n",
    "            columns='article_id',\n",
    "            fill_value=0\n",
    "        )\n",
    "    \n",
    "    def get_recommendations(self, user_id, n_recommendations=10):\n",
    "        \"\"\"Get hybrid recommendations for a user\"\"\"\n",
    "        try:\n",
    "            # Convert user_id to int if it's not already\n",
    "            user_id = int(user_id)\n",
    "            \n",
    "            # Check if user exists\n",
    "            if user_id not in self.user_data['UserId'].unique():\n",
    "                self.logger.error(f\"User {user_id} not found. Available user IDs: {sorted(self.user_data['UserId'].unique())}\")\n",
    "                raise ValueError(f\"User {user_id} not found in the dataset\")\n",
    "            \n",
    "            # Get user's reading history\n",
    "            user_articles = self.user_data[self.user_data['UserId'] == user_id]\n",
    "            \n",
    "            # Get recommendations\n",
    "            content_recommendations = []\n",
    "            for _, row in user_articles.iterrows():\n",
    "                article_idx = row['article_id']\n",
    "                if article_idx in self.minhashes:\n",
    "                    similar_articles = self.lsh_forest.query(self.minhashes[article_idx], 2)\n",
    "                    content_recommendations.extend(map(int, similar_articles))\n",
    "            \n",
    "            # Get collaborative recommendations\n",
    "            similar_users = []\n",
    "            for other_user in self.user_data['UserId'].unique():\n",
    "                if other_user != user_id:\n",
    "                    other_articles = set(self.user_data[self.user_data['UserId'] == other_user]['article_id'])\n",
    "                    user_articles_set = set(user_articles['article_id'])\n",
    "                    similarity = len(other_articles.intersection(user_articles_set)) / len(other_articles.union(user_articles_set))\n",
    "                    similar_users.append((other_user, similarity))\n",
    "            \n",
    "            similar_users = sorted(similar_users, key=lambda x: x[1], reverse=True)[:5]\n",
    "            \n",
    "            collaborative_recommendations = []\n",
    "            for similar_user, _ in similar_users:\n",
    "                similar_user_articles = self.user_data[\n",
    "                    (self.user_data['UserId'] == similar_user) &\n",
    "                    (self.user_data['Clicked'] == 1)\n",
    "                ]['article_id'].tolist()\n",
    "                collaborative_recommendations.extend(similar_user_articles)\n",
    "            \n",
    "            # Combine recommendations\n",
    "            all_recommendations = set(content_recommendations + collaborative_recommendations)\n",
    "            all_recommendations = all_recommendations - set(user_articles['article_id'])\n",
    "            \n",
    "            scored_recommendations = []\n",
    "            for article_id in all_recommendations:\n",
    "                if article_id not in self.process_data.index:\n",
    "                    continue\n",
    "                    \n",
    "                article_data = self.process_data.loc[article_id]\n",
    "                category_score = len(user_articles[user_articles['article_id'].isin(\n",
    "                    self.process_data[self.process_data['category'] == article_data['category']].index\n",
    "                )]) / len(user_articles)\n",
    "                popularity_score = len(self.user_data[self.user_data['article_id'] == article_id]) / len(self.user_data)\n",
    "                final_score = 0.7 * category_score + 0.3 * popularity_score\n",
    "                \n",
    "                scored_recommendations.append({\n",
    "                    'article_id': article_id,\n",
    "                    'headline': article_data['headline'],\n",
    "                    'link': article_data['News_Link'],\n",
    "                    'category': article_data['category'],\n",
    "                    'score': final_score\n",
    "                })\n",
    "            \n",
    "            # Sort and return top recommendations\n",
    "            scored_recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "            return scored_recommendations[:n_recommendations]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting recommendations: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data...\n",
      "INFO:__main__:Loaded 1310 user interactions and 2956 articles\n",
      "INFO:__main__:Initializing components...\n",
      "INFO:__main__:Preparing data structures...\n",
      "INFO:__main__:Processing 2956 articles...\n",
      "INFO:__main__:Processed 100/2956 articles\n",
      "INFO:__main__:Processed 200/2956 articles\n",
      "INFO:__main__:Processed 300/2956 articles\n",
      "INFO:__main__:Processed 400/2956 articles\n",
      "INFO:__main__:Processed 500/2956 articles\n",
      "INFO:__main__:Processed 600/2956 articles\n",
      "INFO:__main__:Processed 700/2956 articles\n",
      "INFO:__main__:Processed 800/2956 articles\n",
      "INFO:__main__:Processed 900/2956 articles\n",
      "INFO:__main__:Processed 1000/2956 articles\n",
      "INFO:__main__:Processed 1100/2956 articles\n",
      "INFO:__main__:Processed 1200/2956 articles\n",
      "INFO:__main__:Processed 1300/2956 articles\n",
      "INFO:__main__:Processed 1400/2956 articles\n",
      "INFO:__main__:Processed 1500/2956 articles\n",
      "INFO:__main__:Processed 1600/2956 articles\n",
      "INFO:__main__:Processed 1700/2956 articles\n",
      "INFO:__main__:Processed 1800/2956 articles\n",
      "INFO:__main__:Processed 1900/2956 articles\n",
      "INFO:__main__:Processed 2000/2956 articles\n",
      "INFO:__main__:Processed 2100/2956 articles\n",
      "INFO:__main__:Processed 2200/2956 articles\n",
      "INFO:__main__:Processed 2300/2956 articles\n",
      "INFO:__main__:Processed 2400/2956 articles\n",
      "INFO:__main__:Processed 2500/2956 articles\n",
      "INFO:__main__:Processed 2600/2956 articles\n",
      "INFO:__main__:Processed 2700/2956 articles\n",
      "INFO:__main__:Processed 2800/2956 articles\n",
      "INFO:__main__:Processed 2900/2956 articles\n",
      "INFO:__main__:Creating user-article interaction matrix...\n",
      "INFO:__main__:Initialization complete!\n",
      "INFO:__main__:\n",
      "Debug Information:\n",
      "INFO:__main__:Number of unique users: 19\n",
      "INFO:__main__:User ID range: 1 to 19\n",
      "INFO:__main__:User ID data type: int64\n",
      "INFO:__main__:Number of articles: 2956\n",
      "INFO:__main__:\n",
      "First few User IDs:\n",
      "INFO:__main__:0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: UserId, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommended Articles for User 6:\n",
      "\n",
      "1. Switch Mobility to build advanced manufacturing facility in Spain\n",
      "   Category: business\n",
      "   Link: https://www.thehindu.com/business/switch-mobility-to-build-advanced-manufacturing-facility-in-spain/article65245728.ece\n",
      "   Score: 0.211\n",
      "\n",
      "2. IndiGo ends streak of quarterly losses; posts Rs 130 crore net profit in Q3 [details inside]\n",
      "   Category: business\n",
      "   Link: https://www.ibtimes.co.in/indigo-ends-streak-quarterly-losses-posts-rs-130-crore-net-profit-q3-details-inside-845407\n",
      "   Score: 0.211\n",
      "\n",
      "3. No rationale for lowering 30% tax on crypto profits: Revenue Secretary Tarun Bajaj\n",
      "   Category: business\n",
      "   Link: https://www.thehindu.com/business/budget/no-rationale-for-lowering-30-tax-on-crypto-profits-bajaj/article38372516.ece\n",
      "   Score: 0.211\n",
      "\n",
      "4. Air India: History reveals iconic rise but fall in wrong hands [Chronology]\n",
      "   Category: business\n",
      "   Link: https://www.ibtimes.co.in/air-india-history-reveals-iconic-rise-fall-wrong-hands-chronology-845154\n",
      "   Score: 0.211\n",
      "\n",
      "5. Union Budget 2022 | Trade unions term it ‘anti-people’\n",
      "   Category: business\n",
      "   Link: https://www.thehindu.com/business/budget/union-budget-2022-trade-unions-term-it-anti-people/article38362026.ece\n",
      "   Score: 0.211\n",
      "\n",
      "6. Russia Resumes Rough Diamond Exports To India After Brief Hiatus Due To West's Sanctions\n",
      "   Category: business\n",
      "   Link: https://www.republicworld.com/business-news/international-business/russia-resumes-rough-diamond-exports-to-india-after-brief-hiatus-due-to-wests-sanctions-articleshow.html\n",
      "   Score: 0.211\n",
      "\n",
      "7. Union Budget 2022 | MGNREGA budget slashed 25% amid high rural unemployment\n",
      "   Category: business\n",
      "   Link: https://www.thehindu.com/business/budget/union-budget-2022-mgnrega-budget-slashed-25-amid-high-rural-unemployment/article38359937.ece\n",
      "   Score: 0.211\n",
      "\n",
      "8. Sensex crashes by 1,800 pts as Russia announces military operations against Ukraine\n",
      "   Category: business\n",
      "   Link: https://www.ibtimes.co.in/sensex-crashes-by-1800-pts-russia-announces-military-operations-against-ukraine-846021\n",
      "   Score: 0.210\n",
      "\n",
      "9. Buying home on loan made easy; SBI reduces interest rates, waives off processing fees [details]\n",
      "   Category: business\n",
      "   Link: https://www.ibtimes.co.in/buying-home-loan-made-easy-sbi-reduces-interest-rates-waives-off-processing-fees-details-833746\n",
      "   Score: 0.210\n",
      "\n",
      "10. Beer hug in times of distancing; Japanese Kirin bets $30mn on Indian Bira\n",
      "   Category: business\n",
      "   Link: https://www.ibtimes.co.in/beer-hug-times-distancing-japanese-kirin-bets-30mn-indian-bira-831824\n",
      "   Score: 0.210\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bot\n",
    "try:\n",
    "    bot = NewsRecommendationBot(\n",
    "        user_data_path='UserProfileData.csv',\n",
    "        process_data_path='Processes_data.csv'\n",
    "    )\n",
    "    \n",
    "    # Get recommendations for a user\n",
    "    user_id = 6  # or whichever user ID you want to test\n",
    "    recommendations = bot.get_recommendations(user_id=user_id)\n",
    "    \n",
    "    if recommendations:\n",
    "        print(f\"\\nRecommended Articles for User {user_id}:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"\\n{i}. {rec['headline']}\")\n",
    "            print(f\"   Category: {rec['category']}\")\n",
    "            print(f\"   Link: {rec['link']}\")\n",
    "            print(f\"   Score: {rec['score']:.3f}\")\n",
    "    else:\n",
    "        print(\"No recommendations found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
